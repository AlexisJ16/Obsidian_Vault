---
id: ZK020
tÃ­tulo: "Arquitectura RAG: GeneraciÃ³n Aumentada por RecuperaciÃ³n"
fecha_creaciÃ³n: 2026-02-22
Ãºltima_modificaciÃ³n: 2026-02-22
tags:
  - nota-permanente
  - rag
  - llm
  - ia-generativa
  - arquitectura
  - vector-db
Ã¡rea: IA & LLMs
fuentes:
  - "Taller 1 - AnÃ¡lisis de Datos I - Icesi"
  - "Deployment of Machine Learning Models"
relacionadas:
  - "[[ZK021 - LLMs y Modelos de Lenguaje]]"
  - "[[ZK010 - CRISP-DM]]"
  - "[[ZK012 - Tipos de AnalÃ­tica]]"
estado: consolidada
---

# Arquitectura RAG: GeneraciÃ³n Aumentada por RecuperaciÃ³n

## ğŸ’¡ Idea central

> RAG (Retrieval-Augmented Generation) resuelve el problema fundamental de los LLMs: las alucinaciones y el desconocimiento del dominio especÃ­fico. En lugar de confiar solo en el conocimiento preentrenado, RAG **recupera documentos relevantes** de una base de datos vectorial y los inyecta como contexto al LLM antes de generar la respuesta.

---

## ğŸ“– Componentes de una Arquitectura RAG

```
Usuario
   â”‚ Query
   â–¼
[1] Embedding del Query â†’ Vector de alta dimensionalidad
   â”‚
   â–¼
[2] Vector DB (BÃºsqueda semÃ¡ntica)
   â”‚   â”Œâ”€â”€â”€ Pinecone / Weaviate / pgvector
   â”‚   â””â”€â”€â”€ Corpus pre-indexado (PDF â†’ chunks â†’ embeddings)
   â”‚ Top-K documentos relevantes
   â–¼
[3] Prompt Engineering
   â”‚   Context = Query + Documentos recuperados
   â–¼
[4] LLM (GPT-4 / Claude / Llama)
   â”‚ Respuesta fundamentada en los documentos
   â–¼
Usuario â† Respuesta con citas verificables
```

---

## ğŸ“– Por quÃ© RAG sobre Fine-tuning

| Aspecto | RAG | Fine-tuning |
|---------|-----|-------------|
| **ActualizaciÃ³n de conocimiento** | InstantÃ¡nea (indexar nuevos docs) | Requiere re-entrenamiento |
| **Costo computacional** | Bajo | Alto |
| **Trazabilidad** | Alta (cita fuentes) | Baja (conocimiento difuso) |
| **Privacidad** | Documentos quedan locales | Datos en el entrenamiento |
| **Ideal para** | Dominios especÃ­ficos con docs | Cambios de estilo/comportamiento |

**ConclusiÃ³n:** Para el dominio legal colombiano, RAG es superior porque:
- El corpus jurÃ­dico se actualiza constantemente (nuevas sentencias)
- La trazabilidad legal es crÃ­tica (citar jurisprudencia exacta)
- Los datos de clientes son PII y no pueden entrenarse en modelos externos

---

## ğŸ“– Pipeline de Ingesta (Data Engineering)

```
PDF / Documentos legales
   â†“ OCR (si son escaneados)
Texto crudo
   â†“ Chunking (dividir en fragmentos ~512 tokens)
Chunks
   â†“ Embedding model (OpenAI text-embedding-3, o local)
Vectores
   â†“ Indexar
Base de datos vectorial (pgvector o Pinecone)
```

---

## ğŸ“– Consideraciones para el contexto legal colombiano

1. **Gobernanza:** Ley 1581/2012 (Habeas Data) â€” los datos PII se procesan localmente
2. **Chunking estratÃ©gico:** ArtÃ­culos de ley = un chunk. Sentencias = por ratio/considerando
3. **PriorizaciÃ³n del corpus:** 40% Derecho Civil, 20% Laboral â†’ indexar primero estas Ã¡reas
4. **Idioma:** Embeddings en espaÃ±ol nativo (multilingual-e5-large o similar)

---

## ğŸ”— Conexiones

- [[ZK021 - LLMs y Modelos de Lenguaje]]
- [[ZK010 - CRISP-DM]]
- [[ZK012 - Tipos de AnalÃ­tica]]
- [[02 - Proyectos/Carrillo Abogados/Asistente Virtual Legal]]
